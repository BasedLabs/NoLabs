{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KFX6O6KMu5y",
        "outputId": "d76bbab4-6b86-4bf1-e6be-867c616fd935"
      },
      "outputs": [],
      "source": [
        "!pip install unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMwmzpiqM7cf",
        "outputId": "e754b334-977d-40b2-bcce-950cd69407e7"
      },
      "outputs": [],
      "source": [
        "!unrar x training_testing_datasets.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5vLVTFKM9us",
        "outputId": "aa6a12cb-ddaa-4083-9887-d90a8ddaad63"
      },
      "outputs": [],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gib2liMX7D_6",
        "outputId": "cde39acb-ba77-40d9-d75d-e01ff6d7bcf0"
      },
      "outputs": [],
      "source": [
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiif-f_F7Pd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from tqdm import tqdm\n",
        "\n",
        "from esm import Alphabet, FastaBatchedDataset, ProteinBertModel, pretrained, MSATransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkBzl1qJNcFR"
      },
      "outputs": [],
      "source": [
        "model_location = \"esm2_t30_150M_UR50D\"\n",
        "include = \"mean\"\n",
        "repr_layers = [-1]\n",
        "truncation_seq_length = 1022\n",
        "nogpu = False\n",
        "toks_per_batch = 4096\n",
        "\n",
        "config = Namespace(\n",
        "    model_location = model_location,\n",
        "    fasta_file = Path(\"/content/training.fasta\"),\n",
        "    repr_layers = repr_layers,\n",
        "    include = include,\n",
        "    output_dir = Path(\"/content/train_embeddings_150M/\"),\n",
        "    truncation_seq_length = truncation_seq_length,\n",
        "    nogpu = nogpu,\n",
        "    toks_per_batch = toks_per_batch\n",
        ")\n",
        "\n",
        "test_config = Namespace(\n",
        "    model_location = model_location,\n",
        "    fasta_file = Path(\"/content/testing.fasta\"),\n",
        "    repr_layers = repr_layers,\n",
        "    include = include,\n",
        "    output_dir = Path(\"/content/test_embeddings_150M/\"),\n",
        "    truncation_seq_length = truncation_seq_length,\n",
        "    nogpu = nogpu,\n",
        "    toks_per_batch = toks_per_batch\n",
        ")\n",
        "\n",
        "\n",
        "def run(args):\n",
        "    model, alphabet = pretrained.load_model_and_alphabet(args.model_location)\n",
        "    model.eval()\n",
        "    if isinstance(model, MSATransformer):\n",
        "        raise ValueError(\n",
        "            \"This script currently does not handle models with MSA input (MSA Transformer).\"\n",
        "        )\n",
        "    if torch.cuda.is_available() and not args.nogpu:\n",
        "        model = model.cuda()\n",
        "        print(\"Transferred model to GPU\")\n",
        "\n",
        "    dataset = FastaBatchedDataset.from_file(args.fasta_file)\n",
        "    batches = dataset.get_batch_indices(args.toks_per_batch, extra_toks_per_seq=1)\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, collate_fn=alphabet.get_batch_converter(args.truncation_seq_length), batch_sampler=batches\n",
        "    )\n",
        "    print(f\"Read {args.fasta_file} with {len(dataset)} sequences\")\n",
        "\n",
        "    args.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    return_contacts = \"contacts\" in args.include\n",
        "\n",
        "    assert all(-(model.num_layers + 1) <= i <= model.num_layers for i in args.repr_layers)\n",
        "    repr_layers = [(i + model.num_layers + 1) % (model.num_layers + 1) for i in args.repr_layers]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (labels, strs, toks) in tqdm(enumerate(data_loader)):\n",
        "\n",
        "            if torch.cuda.is_available() and not args.nogpu:\n",
        "                toks = toks.to(device=\"cuda\", non_blocking=True)\n",
        "\n",
        "            out = model(toks, repr_layers=repr_layers, return_contacts=return_contacts)\n",
        "\n",
        "            logits = out[\"logits\"].to(device=\"cpu\")\n",
        "            representations = {\n",
        "                layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()\n",
        "            }\n",
        "            if return_contacts:\n",
        "                contacts = out[\"contacts\"].to(device=\"cpu\")\n",
        "\n",
        "            for i, label in enumerate(labels):\n",
        "                args.output_file = args.output_dir / f\"{label}.pt\"\n",
        "                args.output_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "                result = {\"label\": label}\n",
        "                truncate_len = min(args.truncation_seq_length, len(strs[i]))\n",
        "                # Call clone on tensors to ensure tensors are not views into a larger representation\n",
        "                # See https://github.com/pytorch/pytorch/issues/1995\n",
        "                if \"per_tok\" in args.include:\n",
        "                    result[\"representations\"] = {\n",
        "                        layer: t[i, 1 : truncate_len + 1].clone()\n",
        "                        for layer, t in representations.items()\n",
        "                    }\n",
        "                if \"mean\" in args.include:\n",
        "                    result[\"mean_representations\"] = {\n",
        "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
        "                        for layer, t in representations.items()\n",
        "                    }\n",
        "                if \"bos\" in args.include:\n",
        "                    result[\"bos_representations\"] = {\n",
        "                        layer: t[i, 0].clone() for layer, t in representations.items()\n",
        "                    }\n",
        "                if return_contacts:\n",
        "                    result[\"contacts\"] = contacts[i, : truncate_len, : truncate_len].clone()\n",
        "\n",
        "                torch.save(\n",
        "                    result,\n",
        "                    args.output_file,\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xci60amV7ukQ",
        "outputId": "26126c09-2997-4e41-c5b2-e2b83312567a"
      },
      "outputs": [],
      "source": [
        "run(config)\n",
        "run(test_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjgwe-T09UCc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "class EmbeddingsDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.file_list = [f for f in os.listdir(self.root_dir) if f.endswith(\".pt\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_list[idx]\n",
        "        full_path = self.root_dir / file_name\n",
        "        embeddings = torch.load(full_path)['mean_representations'][30]\n",
        "\n",
        "        class_label = int(\"|1|\" in file_name)\n",
        "\n",
        "        return embeddings, class_label\n",
        "\n",
        "# Example usage\n",
        "data_directory = \"/content/train_embeddings_150M\"\n",
        "dataset = EmbeddingsDataset(data_directory)\n",
        "\n",
        "# Create a DataLoader for the dataset\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMLE3l2SAXwz",
        "outputId": "be8b6079-9b3b-41d9-8d3a-7f8b23e77f89"
      },
      "outputs": [],
      "source": [
        "# Example: Iterate through batches in the DataLoader\n",
        "for batch_embeddings, batch_labels in dataloader:\n",
        "    # Perform your operations on the embeddings and labels\n",
        "    print(\"Batch embeddings shape:\", batch_embeddings.shape)\n",
        "    print(\"Batch labels:\", batch_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifhhgJZbAb9s",
        "outputId": "19baa9ce-7013-4d92-c995-43e88fde4d43"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define the MLP binary classifier with dropout\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        for size in hidden_sizes:\n",
        "            layers.append(nn.Linear(prev_size, size))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))  # Add dropout layer\n",
        "            prev_size = size\n",
        "        self.hidden_layers = nn.Sequential(*layers)\n",
        "        self.output_layer = nn.Linear(prev_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layers(x)\n",
        "        x = self.output_layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "data_directory = \"/content/train_embeddings_150M\"\n",
        "dataset = EmbeddingsDataset(data_directory)  # Assuming you have defined the EmbeddingsDataset class\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader instances for training and validation\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = dataset[0][0].shape[0]  # Assuming the embeddings have a fixed size\n",
        "hidden_sizes = [320, 160, 80, 40]  # Additional hidden layers\n",
        "output_size = 1  # Binary classification\n",
        "learning_rate = 0.003\n",
        "dropout_rate = 0.2\n",
        "\n",
        "model = MLPClassifier(input_size, hidden_sizes, output_size, dropout_rate)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Number of epochs to wait without improvement\n",
        "min_val_loss = np.inf\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 6\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_embeddings, batch_labels in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(batch_embeddings.float())  # Convert embeddings to float\n",
        "        loss = criterion(outputs.squeeze(), batch_labels.float())  # Squeeze to match dimensions\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_embeddings, batch_labels in val_dataloader:\n",
        "            outputs = model(batch_embeddings.float())\n",
        "            loss = criterion(outputs.squeeze(), batch_labels.float())\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            predicted_labels = (outputs >= 0.5).long()\n",
        "            correct_predictions += (predicted_labels == batch_labels).sum().item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    val_accuracy = correct_predictions / len(val_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < min_val_loss:\n",
        "        min_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuTkFq21aU-9"
      },
      "source": [
        "# Benhcmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlIXDtAgHk7K"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "140FBzx3aQfM",
        "outputId": "8daa4770-c5f4-4b10-a72c-aaeaefba7c53"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Load the test dataset\n",
        "test_directory = \"/content/test_embeddings_150M\"\n",
        "test_dataset = EmbeddingsDataset(test_directory)  # Assuming you have defined the EmbeddingsDataset class\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)  # Use an appropriate batch size\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_embeddings, batch_labels in test_dataloader:\n",
        "        outputs = model(batch_embeddings.float())\n",
        "        predicted_batch = (outputs >= 0.5).long()\n",
        "\n",
        "        true_labels.extend(batch_labels.tolist())\n",
        "        predicted_labels.extend(predicted_batch.tolist())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx_odll9gvpO"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/solubility_model.pth\"\n",
        "torch.save(model.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOmwUTZWjDds"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}